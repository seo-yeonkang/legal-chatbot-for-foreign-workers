# =============================================================================
# utils/embedding_index.py - ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤
# =============================================================================

from sentence_transformers import SentenceTransformer
import faiss
import json
import pickle
import numpy as np
import streamlit as st
from pathlib import Path
import config

@st.cache_resource
def load_embeddings_and_index():
    """
    ì–¸ì–´ë³„ ì„ë² ë”© ëª¨ë¸ê³¼ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    
    Returns:
        tuple: (embedding_model, cn_index, cn_passages, cn_metadata, vn_index, vn_passages, vn_metadata)
    """
    try:
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ê³µí†µ)
        embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)
        
        # ì¤‘êµ­ì–´ ì¸ë±ìŠ¤ ë¡œë“œ
        cn_index, cn_passages, cn_metadata = load_language_index(
            embedding_model, 
            'zh',
            config.CN_LAW_DATA_PATH,
            config.CN_FAISS_INDEX_PATH,
            config.CN_PASSAGES_PATH
        )
        
        # ë² íŠ¸ë‚¨ì–´ ì¸ë±ìŠ¤ ë¡œë“œ
        vn_index, vn_passages, vn_metadata = load_language_index(
            embedding_model,
            'vi', 
            config.VN_LAW_DATA_PATH,
            config.VN_FAISS_INDEX_PATH,
            config.VN_PASSAGES_PATH
        )
        
        return embedding_model, cn_index, cn_passages, cn_metadata, vn_index, vn_passages, vn_metadata
        
    except Exception as e:
        st.error(f"ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None, None, None, None, None, None

def load_language_index(embedding_model, language, jsonl_path, faiss_path, passages_path):
    """íŠ¹ì • ì–¸ì–´ì˜ ì¸ë±ìŠ¤ ë¡œë“œ"""
    try:
        # ìºì‹œëœ ì¸ë±ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        if faiss_path.exists() and passages_path.exists():
            return load_cached_language_index(embedding_model, language, faiss_path, passages_path)
        
        # ìƒˆë¡œ ì¸ë±ìŠ¤ ìƒì„±
        return create_new_language_index(embedding_model, language, jsonl_path, faiss_path, passages_path)
        
    except Exception as e:
        st.warning(f"{language} ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None, None

def load_cached_language_index(embedding_model, language, faiss_path, passages_path):
    """ìºì‹œëœ ì–¸ì–´ë³„ FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
    try:
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_index = faiss.read_index(str(faiss_path))
        
        # íŒ¨ì‹œì§€ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(passages_path, 'rb') as f:
            data = pickle.load(f)
            passages = data['passages']
            metadata = data['metadata']
        
        lang_name = "ì¤‘êµ­ì–´" if language == "zh" else "ë² íŠ¸ë‚¨ì–´"
        st.success(f"âœ… {lang_name} ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(passages)}ê°œ ë¬¸ì„œ")
        return faiss_index, passages, metadata
        
    except Exception as e:
        st.warning(f"{language} ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {str(e)}")
        return None, None, None

def create_new_language_index(embedding_model, language, jsonl_path, faiss_path, passages_path):
    """ìƒˆë¡œìš´ ì–¸ì–´ë³„ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    try:
        # JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        passages, metadata = load_jsonl_data(jsonl_path)
        
        if not passages:
            st.error(f"{language} ë²•ë¥  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
            return None, None, None
        
        # ì„ë² ë”© ìƒì„±
        lang_name = "ì¤‘êµ­ì–´" if language == "zh" else "ë² íŠ¸ë‚¨ì–´"
        st.info(f"ğŸ“š {lang_name} ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        embeddings = embedding_model.encode(
            passages, 
            show_progress_bar=True,
            batch_size=32
        )
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        faiss_index = faiss.IndexFlatL2(dimension)
        faiss_index.add(embeddings.astype('float32'))
        
        # ìºì‹œ ì €ì¥
        save_language_index(faiss_index, passages, metadata, faiss_path, passages_path)
        
        st.success(f"âœ… {lang_name} ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(passages)}ê°œ ë¬¸ì„œ")
        return faiss_index, passages, metadata
        
    except Exception as e:
        st.error(f"{language} ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return None, None, None

def load_jsonl_data(jsonl_path):
    """JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    passages = []
    metadata = []
    
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    # ë²ˆì—­ëœ ë¬¸ì¥ ì‚¬ìš© (Trans_Sentence í•„ë“œ)
                    if "Trans_Sentence" in data:
                        passages.append(data["Trans_Sentence"])
                        metadata.append(data)
                    else:
                        st.warning(f"Line {line_num}: 'Trans_Sentence' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                except json.JSONDecodeError:
                    st.warning(f"Line {line_num}: JSON íŒŒì‹± ì˜¤ë¥˜")
                    continue
        
        return passages, metadata
        
    except FileNotFoundError:
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
        return [], []

def save_language_index(faiss_index, passages, metadata, faiss_path, passages_path):
    """ì–¸ì–´ë³„ ì„ë² ë”©ê³¼ ì¸ë±ìŠ¤ë¥¼ ìºì‹œì— ì €ì¥"""
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        config.DATA_DIR.mkdir(exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(faiss_index, str(faiss_path))
        
        # íŒ¨ì‹œì§€ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(passages_path, 'wb') as f:
            pickle.dump({
                'passages': passages,
                'metadata': metadata
            }, f)
        
        st.success("ğŸ“ ì¸ë±ìŠ¤ê°€ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        st.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def search_similar_passages(embedding_model, faiss_index, passages, query: str, k: int = None):
    """
    ìœ ì‚¬í•œ ë²•ë¥  ì¡°ë¬¸ ê²€ìƒ‰ (ì–¸ì–´ë³„ ì¸ë±ìŠ¤ ì‚¬ìš©)
    
    Args:
        embedding_model: ì„ë² ë”© ëª¨ë¸
        faiss_index: í•´ë‹¹ ì–¸ì–´ì˜ FAISS ì¸ë±ìŠ¤
        passages: í•´ë‹¹ ì–¸ì–´ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ë“¤
        query (str): ê²€ìƒ‰ ì¿¼ë¦¬
        k (int): ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
    
    Returns:
        list: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
    """
    if k is None:
        k = config.MAX_RETRIEVED_DOCS
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = embedding_model.encode([query])
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        distances, indices = faiss_index.search(query_embedding.astype('float32'), k)
        
        # ê²°ê³¼ ë°˜í™˜
        retrieved_docs = []
        for i, idx in enumerate(indices[0]):
            if idx < len(passages):
                retrieved_docs.append({
                    'text': passages[idx],
                    'score': float(distances[0][i]),
                    'index': int(idx)
                })
        
        return retrieved_docs
        
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return []
