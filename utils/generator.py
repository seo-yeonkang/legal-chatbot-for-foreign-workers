# =============================================================================
# utils/generator.py - ë‹µë³€ ìƒì„±
# =============================================================================

from transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM
import torch
import streamlit as st
import config
import gdown
import zipfile
import os
from pathlib import Path
import faiss
import pickle
import json

# SentenceTransformerëŠ” í•„ìš”í•  ë•Œë§Œ import (circular import ë°©ì§€)
def get_sentence_transformer():
    """SentenceTransformer ì§€ì—° import"""
    from sentence_transformers import SentenceTransformer
    return SentenceTransformer

READY_MARKER = config.CHINESE_MODEL_LOCAL_PATH / ".ready"   # âœ… 1íšŒ ì™„ë£Œí‘œì‹œ

def download_chinese_model_from_gdrive():
    """
    â€¢ models/chinese_model/ ê°€ ì—†ìœ¼ë©´ Drive í´ë”(ID)ì—ì„œ ë‹¤ìš´ë¡œë“œ
    â€¢ ì™„ë£Œ í›„ '.ready' íŒŒì¼ ìƒì„± â†’ ë‹¤ìŒ ì‹¤í–‰ë¶€í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ
    """
    # â‘  ì´ë¯¸ ì¤€ë¹„ëìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
    if READY_MARKER.exists():
        return str(config.CHINESE_MODEL_LOCAL_PATH)

    try:
        st.info("ğŸ“¥ ì¤‘êµ­ì–´ ëª¨ë¸ì´ ì—†ì–´ì„œ Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤â€¦")

        # models/ ë””ë ‰í„°ë¦¬
        config.MODELS_DIR.mkdir(exist_ok=True, parents=True)

        # â‘¡ Drive í´ë” ì „ì²´ ë‹¤ìš´ë¡œë“œ â†’ models/ í•˜ìœ„ì—
        folder_url = f"https://drive.google.com/drive/folders/{config.CHINESE_MODEL_GDRIVE_ID}"
        gdown.download_folder(
            folder_url,
            output=str(config.CHINESE_MODEL_LOCAL_PATH),  # âœ… ë°”ë¡œ chinese_model í´ë”
            quiet=False,
            use_cookies=False
        )

        # â‘¢ í´ë”ëª… ì •ê·œí™” â†’ chinese_model
        if not config.CHINESE_MODEL_LOCAL_PATH.exists():
            for p in config.MODELS_DIR.iterdir():
                if p.is_dir() and p.name != "chinese_model":
                    p.rename(config.CHINESE_MODEL_LOCAL_PATH)
                    break

        # â‘£ ì„œë¸Œí´ë”ì— ìˆ¨ì–´ìˆëŠ” ê°€ì¤‘ì¹˜(.bin/.safetensors) ëŒì–´ì˜¬ë¦¬ê¸°
        for root, _, files in os.walk(config.CHINESE_MODEL_LOCAL_PATH):
            for fn in files:
                if fn.endswith((".bin", ".safetensors")):
                    src = Path(root) / fn
                    dst = config.CHINESE_MODEL_LOCAL_PATH / fn
                    if not dst.exists():
                        src.replace(dst)

        # â‘¤ ë§ˆì»¤ ìƒì„± â†’ ì´í›„ ì¬ë‹¤ìš´ë¡œë“œ ì—†ìŒ
        READY_MARKER.touch()
        st.success("âœ… ì¤‘êµ­ì–´ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (ìºì‹œë¨)!")
        return str(config.CHINESE_MODEL_LOCAL_PATH)

    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
        return None
        

@st.cache_resource
def load_generation_models():
    """ìƒì„± ëª¨ë¸ë“¤ ë¡œë“œ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ + ì™¸ë¶€ ëª¨ë¸)"""
    try:
        # ì¤‘êµ­ì–´ ëª¨ë¸ - êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ
        chinese_model_path = download_chinese_model_from_gdrive()
        if chinese_model_path is None:
            st.error("ì¤‘êµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return None, None
            
        chinese_model = MBartForConditionalGeneration.from_pretrained(
             chinese_model_path,
             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
             device_map="auto" if torch.cuda.is_available() else None,
             local_files_only=True
         )
        
        # ë² íŠ¸ë‚¨ì–´ ëª¨ë¸ - ì™¸ë¶€ ëª¨ë¸ (HuggingFace Hub)
        vietnamese_model = AutoModelForSeq2SeqLM.from_pretrained(
            config.VIETNAMESE_MODEL,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        st.success("âœ… ëª¨ë“  ìƒì„± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return chinese_model, vietnamese_model
        
    except Exception as e:
        st.error(f"ìƒì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

def generate_answer(prompt: str, model, tokenizer, max_length: int = None, temperature: float = None):
    """
    ë‹µë³€ ìƒì„±
    
    Args:
        prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
        model: ìƒì„± ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length (int): ìµœëŒ€ ìƒì„± ê¸¸ì´
        temperature (float): ìƒì„± ì˜¨ë„
    
    Returns:
        str: ìƒì„±ëœ ë‹µë³€
    """
    
    if max_length is None:
        max_length = config.MAX_GENERATION_LENGTH
    if temperature is None:
        temperature = config.TEMPERATURE
    
    try:
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=config.MAX_INPUT_LENGTH,
            padding=True
        )
        
        inputs.pop("token_type_ids", None)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=config.TOP_P,
                do_sample=True if temperature > 0 else False,
                num_beams=3 if temperature == 0 else 1,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(
            outputs[0], 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° (mT5ì˜ ê²½ìš°)
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
        
        return generated_text
        
    except Exception as e:
        st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def generate_streaming_answer(prompt: str, model, tokenizer, max_length: int = None):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì¶œë ¥)
    
    Args:
        prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
        model: ìƒì„± ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length (int): ìµœëŒ€ ìƒì„± ê¸¸ì´
    
    Yields:
        str: ìƒì„±ë˜ëŠ” í† í°ë“¤
    """
    
    if max_length is None:
        max_length = config.MAX_GENERATION_LENGTH
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=config.MAX_INPUT_LENGTH)
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        inputs.pop("token_type_ids", None)
        
        with torch.no_grad():
            # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì„¤ì •
            generated_ids = inputs["input_ids"]
            
            for _ in range(max_length):
                outputs = model(input_ids=generated_ids, attention_mask=torch.ones_like(generated_ids))
                next_token_logits = outputs.logits[:, -1, :]
                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)
                
                # ìƒˆ í† í° ë””ì½”ë”©
                new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)
                yield new_token
                
                # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                if next_token_id.item() == tokenizer.eos_token_id:
                    break
                    
    except Exception as e:
        yield f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì˜¤ë¥˜: {str(e)}"
