# =============================================================================
# utils/generator.py - ë‹µë³€ ìƒì„±
# =============================================================================

from transformers import AutoModelForSeq2SeqLM, SentenceTransformer
import torch
import streamlit as st
import config
import gdown
import zipfile
import os
from pathlib import Path
import faiss
import pickle
import json

def download_chinese_model_from_gdrive():
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ì¤‘êµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ì´ë¯¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if config.CHINESE_MODEL_LOCAL_PATH.exists():
            st.info("ğŸ”„ ì¤‘êµ­ì–´ ëª¨ë¸ì´ ì´ë¯¸ ë¡œì»¬ì— ìˆìŠµë‹ˆë‹¤.")
            return str(config.CHINESE_MODEL_LOCAL_PATH)
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        config.MODELS_DIR.mkdir(exist_ok=True)
        
        # êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ í´ë” ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ“¥ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ì¤‘êµ­ì–´ ëª¨ë¸ í´ë”ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ë°©ë²• 1: í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í–ˆë‹¤ë©´
        # url = f"https://drive.google.com/uc?id={config.CHINESE_MODEL_GDRIVE_ID}"
        # gdown.download(url, str(config.CHINESE_MODEL_ZIP_PATH), quiet=False)
        
        # ë°©ë²• 2: í´ë” ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)
        folder_url = f"https://drive.google.com/drive/folders/{config.CHINESE_MODEL_GDRIVE_ID}"
        gdown.download_folder(folder_url, output=str(config.MODELS_DIR), quiet=False)
        
        # ë‹¤ìš´ë¡œë“œëœ í´ë”ëª…ì„ chinese_modelë¡œ ë³€ê²½ (í•„ìš”ì‹œ)
        downloaded_folder = config.MODELS_DIR / "chinese_model"
        if not downloaded_folder.exists():
            # gdownì´ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ í´ë”ë¥¼ ë§Œë“¤ì—ˆì„ ìˆ˜ ìˆìŒ
            for folder in config.MODELS_DIR.iterdir():
                if folder.is_dir() and folder.name != "chinese_model":
                    folder.rename(downloaded_folder)
                    break
        
        st.success("âœ… ì¤‘êµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return str(config.CHINESE_MODEL_LOCAL_PATH)
        
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•: êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ í´ë”ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  models/chinese_model ê²½ë¡œì— ì••ì¶• í•´ì œí•˜ì„¸ìš”.")
        return None

@st.cache_resource
def load_generation_models():
    """ìƒì„± ëª¨ë¸ë“¤ ë¡œë“œ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ + ì™¸ë¶€ ëª¨ë¸)"""
    try:
        # ì¤‘êµ­ì–´ ëª¨ë¸ - êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ
        chinese_model_path = download_chinese_model_from_gdrive()
        if chinese_model_path is None:
            st.error("ì¤‘êµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return None, None
            
        chinese_model = AutoModelForSeq2SeqLM.from_pretrained(
            chinese_model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            local_files_only=True  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
        )
        
        # ë² íŠ¸ë‚¨ì–´ ëª¨ë¸ - ì™¸ë¶€ ëª¨ë¸ (HuggingFace Hub)
        vietnamese_model = AutoModelForSeq2SeqLM.from_pretrained(
            config.VIETNAMESE_MODEL,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        st.success("âœ… ëª¨ë“  ìƒì„± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return chinese_model, vietnamese_model
        
    except Exception as e:
        st.error(f"ìƒì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

def generate_answer(prompt: str, model, tokenizer, max_length: int = None, temperature: float = None):
    """
    ë‹µë³€ ìƒì„±
    
    Args:
        prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
        model: ìƒì„± ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length (int): ìµœëŒ€ ìƒì„± ê¸¸ì´
        temperature (float): ìƒì„± ì˜¨ë„
    
    Returns:
        str: ìƒì„±ëœ ë‹µë³€
    """
    if max_length is None:
        max_length = config.MAX_GENERATION_LENGTH
    if temperature is None:
        temperature = config.TEMPERATURE
    
    try:
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=config.MAX_INPUT_LENGTH,
            padding=True
        )
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=config.TOP_P,
                do_sample=True if temperature > 0 else False,
                num_beams=3 if temperature == 0 else 1,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(
            outputs[0], 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° (mT5ì˜ ê²½ìš°)
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
        
        return generated_text
        
    except Exception as e:
        st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def generate_streaming_answer(prompt: str, model, tokenizer, max_length: int = None):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì¶œë ¥)
    
    Args:
        prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
        model: ìƒì„± ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length (int): ìµœëŒ€ ìƒì„± ê¸¸ì´
    
    Yields:
        str: ìƒì„±ë˜ëŠ” í† í°ë“¤
    """
    if max_length is None:
        max_length = config.MAX_GENERATION_LENGTH
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=config.MAX_INPUT_LENGTH)
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì„¤ì •
            generated_ids = inputs["input_ids"]
            
            for _ in range(max_length):
                outputs = model(input_ids=generated_ids, attention_mask=torch.ones_like(generated_ids))
                next_token_logits = outputs.logits[:, -1, :]
                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)
                
                # ìƒˆ í† í° ë””ì½”ë”©
                new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)
                yield new_token
                
                # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                if next_token_id.item() == tokenizer.eos_token_id:
                    break
                    
    except Exception as e:
        yield f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì˜¤ë¥˜: {str(e)}"
